{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e1c2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import allel\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66634b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~')\n",
    "directory = os.path.join('Imp_Research','Dataset')\n",
    "\n",
    "populations = ['BFcol','BFgam','AOcol','CIcol','CMgam','FRgam',\n",
    "              'GAgam','GHcol','GHgam','GM','GNcol','GNgam','GQgam',\n",
    "              'GW','KE','UGgam']\n",
    "R_SEED = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4fe29470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of Population names with labels\n",
    "populations_labels = {}\n",
    "for i in range(len(populations)):\n",
    "    populations_labels[i] = populations[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802090b",
   "metadata": {},
   "source": [
    "### Functions to Load and Pre-Process the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f27435d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Requirements : NumPy and Scikit-Allel\n",
    "'''\n",
    "\n",
    "class FilterSNP():\n",
    "    def __init__(self,haplotype, POS):\n",
    "        self.haplotype = haplotype\n",
    "        self.POS = POS\n",
    "        self.H = haplotype\n",
    "        self.P = POS\n",
    "        self._removed_maf = None\n",
    "        self._retained_maf = None\n",
    "        self._retained_ld = None\n",
    "        self._removed_ld = None\n",
    "        \n",
    "    def all_filters(self, LD_window_size, LD_overlap_step, MBP_start = 1, MBP_end = 37, MAF_threshold = 5, LD_threshold=.1,LD_iter=1):\n",
    "        print(\"1.) Selecting Mega Base Pairs\")\n",
    "        self.H,self.P = self.get_haplo_MBP(self.H,self.P,start = MBP_start, end = MBP_end)\n",
    "        print(\"MBP selected. Retained Matrix = \", self.H.shape)\n",
    "        print(\"2.) Filtering Rare Allels\")\n",
    "        self.H,self.P = self.filter_MAF(self.H,self.P,threshold = MAF_threshold)\n",
    "        print(\"3.) Performing LD Pruning\")\n",
    "        self.H = self.LD_pruning(self.H, LD_window_size, LD_overlap_step, threshold = LD_threshold, n_iter = LD_iter)\n",
    "        print(\"Retained Matrix = \", self.H.shape)\n",
    "        \n",
    "        return self.H, self.P\n",
    "\n",
    "    def filter_MAF(self,haplo,POS,threshold = 5):\n",
    "        if threshold >= 50 : \n",
    "            print(\"MAF threshold cannot be more than 49%\")\n",
    "            return\n",
    "        samples = haplo.shape[1]\n",
    "        sums = haplo.sum(axis=1)\n",
    "        maf = self.get_MAF(haplo)\n",
    "        #indexes = np.where(maf >= threshold*0.01)[0]\n",
    "        minor = samples*threshold/100\n",
    "        major = samples*(100-threshold)/100\n",
    "        # Selects indexes where allels are >threshold or all 0 and all 1.\n",
    "        indexes = np.where((sums>=minor)& (sums<=major))[0]\n",
    "        print(\"Number of SNPs removed = \",len(haplo)-len(indexes))\n",
    "        print(\"Retaining = \",len(indexes))\n",
    "        self._removed_maf = len(haplo)-len(indexes)\n",
    "        self._retained_maf = len(indexes)\n",
    "        return np.take(haplo,indexes,0), np.take(POS,indexes,0)\n",
    "\n",
    "    # Returns : Array of MAF\n",
    "    def get_MAF(self,haplo):\n",
    "        samples = haplo.shape[1]\n",
    "        sums = haplo.sum(axis=1)\n",
    "        maf = []\n",
    "        for s in sums:\n",
    "            if s != samples or s != 0:\n",
    "                frequency = s/samples\n",
    "                if frequency > 0.5:\n",
    "                    maf.append(1-frequency)\n",
    "                else: \n",
    "                    maf.append(frequency)\n",
    "        return np.array(maf)\n",
    "\n",
    "    def get_MBP(self,POS,start = 1,end = 37):\n",
    "        return np.where(POS[np.where(POS>=1e6)]<=37e6)[0]\n",
    "\n",
    "    def get_haplo_MBP(self,haplotype,POS,start = 1,end = 37):\n",
    "        index = self.get_MBP(POS,start,end)\n",
    "        return np.take(haplotype,index,0),np.take(POS,index,0)\n",
    "    \n",
    "    def LD_pruning(self,gn, size, step, threshold = .1, n_iter=1):\n",
    "        removed = 0\n",
    "        for i in range(n_iter):\n",
    "            loc_unlinked = allel.locate_unlinked(gn, size=size, step=step, threshold=threshold)\n",
    "            n = np.count_nonzero(loc_unlinked)\n",
    "            n_remove = gn.shape[0] - n\n",
    "            removed += n_remove\n",
    "            print('iteration', i+1, 'retaining', n, 'removing', n_remove, 'variants')\n",
    "            gn = gn.compress(loc_unlinked, axis=0)\n",
    "        self._retained_ld = gn.shape[0]\n",
    "        self._removed_ld = removed\n",
    "        return gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4edbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Disclaimer : The class is for personal use. It is not aimed for portability or reusability.\n",
    "\n",
    "Class to load and filter the data from disk.\n",
    "Object Parameters : \n",
    "data_path  -> Path to the data directory\n",
    "\n",
    "load_pop() : Function to load the data\n",
    "params : \n",
    "Populations -> list or array of population names/filenames\n",
    "filtered -> boolean, whethere to filter the data or not\n",
    "combine -> Boolean, to combine the populations (Implement it !!!)\n",
    "\n",
    "returns : A dictionary of haplotype matrix and Position array\n",
    "'''\n",
    "\n",
    "class LoadFilteredPops():\n",
    "    def __init__(self,data_path = None):\n",
    "        self.data_path = data_path\n",
    "        \n",
    "    def load_pop(self,populations,naming='custom',chromo_arms = ['3R'],filtered = True):\n",
    "        if self.data_path is None:\n",
    "            home = os.path.expanduser('~')\n",
    "            directory = os.path.join('Imp_Research','Dataset')\n",
    "        Haplo_pop = {}\n",
    "        POS_pop = {}\n",
    "        for population in populations:\n",
    "            for arm in chromo_arms:\n",
    "                pop_name = population+'.'+arm\n",
    "                if naming == 'custom':\n",
    "                    filename = f'Haplotype.POS.{pop_name}.hd5'\n",
    "                else:\n",
    "                    filename = population\n",
    "                if self.data_path is None:\n",
    "                    data_path = os.path.join(home, directory,\"HDF_Dataset\", filename)\n",
    "                else:\n",
    "                    try:\n",
    "                        data_path = os.path.join(self.data_path,filename)\n",
    "                    except:\n",
    "                        print(\"Cannot resolve Directory path\")\n",
    "                        exit()\n",
    "                print(f'------{pop_name}------\\n')\n",
    "                H = pd.read_hdf(data_path,key='Haplotype').astype('int8').to_numpy().astype('int8')\n",
    "                P = pd.read_hdf(data_path,key='POS').to_numpy()\n",
    "                \n",
    "                if filtered:\n",
    "                    datafilter = FilterSNP(H,P)\n",
    "                    Haplo_pop[pop_name],POS_pop[pop_name] = datafilter.all_filters(LD_window_size = 500,LD_overlap_step = 100,LD_iter = 3)\n",
    "                else: \n",
    "                    Haplo_pop[pop_name],POS_pop[pop_name] = H,P\n",
    "                del H,P     \n",
    "        print(\"Populations loaded !!!\")\n",
    "        return Haplo_pop,POS_pop\n",
    "\n",
    "    \n",
    "'''\n",
    "Function to combine the populations\n",
    "Params\n",
    "Haplo_all : Dictionary containing Haplotype matrix for populations. Key-> pop name; Value -> matrix.\n",
    "            Matrix dimensions must be POS x haplotypes.\n",
    "POS_all : Position array of SNPs\n",
    "filtered : Boolean, to filter after combining or not.\n",
    "get_labels : Boolean, to generate labels\n",
    "\n",
    "returns\n",
    "Haplo_all : ndarray of n x m dimensions. n = length of POS array; m = sum of haplotypes from all populations.\n",
    "POS : SNP Position array\n",
    "labels : list of labels if Labels = []\n",
    "'''\n",
    "def combine_pops(H_all,P_all,filtered = True,get_labels = True):\n",
    "    keys = list(H_all.keys())\n",
    "    H = np.array(H_all[keys[0]])\n",
    "    if len(H_all) > 1:\n",
    "        for i in range(1,len(H_all)):\n",
    "            H = np.append(H,H_all[keys[i]],axis=1)\n",
    "    print('Combined Shape => ',H.shape)\n",
    "    if get_labels:\n",
    "        label = []\n",
    "        for each in keys:\n",
    "            label.extend([each*len(H_all[each][0]))\n",
    "    if filtered == False:\n",
    "        if get_labels:\n",
    "            return H,P_all,label\n",
    "        else:\n",
    "            return H,p_all\n",
    "    else:\n",
    "        datafilter = FilterSNP(H,P_all)\n",
    "        H_filtered,POS_filtered = datafilter.all_filters(LD_window_size = 500,LD_overlap_step = 100,LD_iter = 3)\n",
    "        print('Filtered Shape => ',H_filtered.shape)\n",
    "        if get_labels:\n",
    "            return H_filtered,POS_filtered,label\n",
    "        else:\n",
    "            return H_filtered,POS_filtered\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e9403",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6b7dcd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------BFcol.3R------\n",
      "\n",
      "------BFgam.3R------\n",
      "\n",
      "------AOcol.3R------\n",
      "\n",
      "------CIcol.3R------\n",
      "\n",
      "------CMgam.3R------\n",
      "\n",
      "------FRgam.3R------\n",
      "\n",
      "------GAgam.3R------\n",
      "\n",
      "------GHcol.3R------\n",
      "\n",
      "------GHgam.3R------\n",
      "\n",
      "------GM.3R------\n",
      "\n",
      "------GNcol.3R------\n",
      "\n",
      "------GNgam.3R------\n",
      "\n",
      "------GQgam.3R------\n",
      "\n",
      "------GW.3R------\n",
      "\n",
      "------KE.3R------\n",
      "\n",
      "------UGgam.3R------\n",
      "\n",
      "Populations loaded !!!\n"
     ]
    }
   ],
   "source": [
    "loader = LoadFilteredPops()\n",
    "Haplo_all, POS_all = loader.load_pop(populations,filtered = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60c8e43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Shape =>  (4836295, 2284)\n",
      "1.) Selecting Mega Base Pairs\n",
      "MBP selected. Retained Matrix =  (3651720, 2284)\n",
      "2.) Filtering Rare Allels\n",
      "Number of SNPs removed =  3451543\n",
      "Retaining =  200177\n",
      "3.) Performing LD Pruning\n",
      "iteration 1 retaining 117993 removing 82184 variants\n",
      "iteration 2 retaining 117494 removing 499 variants\n",
      "iteration 3 retaining 117461 removing 33 variants\n",
      "Retained Matrix =  (117461, 2284)\n",
      "Filtered Shape =>  (117461, 2284)\n"
     ]
    }
   ],
   "source": [
    "# Combining and Filtering the SNPs\n",
    "H_allF, POS_allF, pop_labels = combine_pops(Haplo_all,POS_all['BFcol.3R'],filtered = True)\n",
    "\n",
    "# To retain the positions simply pass along a list of index values.\n",
    "# Remove the same indexes as POS hence we will know which indexes to use while infering from models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32da920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Labels\n",
    "\n",
    "labels_all = pd.DataFrame(pop_labels)\n",
    "for i in range(len(populations)):\n",
    "    labels_all[0].replace(populations[i],i,inplace=True)\n",
    "\n",
    "\n",
    "# Name coded labels\n",
    "labels_allname = pd.DataFrame(pop_labels)\n",
    "labels_allname = labels_allname.to_numpy()\n",
    "# Number coded class labels\n",
    "labels_all = labels_all[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "83108e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2284,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "897ef601",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pd.DataFrame(H_13).astype('int8')\n",
    "H.to_hdf('~/Imp_Research/Dataset/Filtered Data/H_13Filtered.hd5',key='Haplotype',format='fixed',mode='w',complevel=9)\n",
    "L = pd.DataFrame(labels_allname)\n",
    "L.to_csv('~/Imp_Research/Dataset/Filtered Data/Labels_name13Filtered.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d474e5",
   "metadata": {},
   "source": [
    "#### Removing GQgam, GHgam and GNcol due to small sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c2814d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaples to be removed (50,)\n",
      "Samples retained  (117461, 2234)\n"
     ]
    }
   ],
   "source": [
    "H_13 = H_allF.copy()\n",
    "\n",
    "# finding the indexes of the samples of given pops.\n",
    "remove_pop = np.where((labels_allname == ['GQgam','GHgam','GNcol']))\n",
    "print(\"Smaples to be removed\",remove_pop[0].shape)\n",
    "\n",
    "# removing the samples\n",
    "H_13 = np.delete(H_13,remove_pop[0],1)\n",
    "print(\"Samples retained \",H_13.shape)\n",
    "\n",
    "# Generating new labels for remaining 13 pops\n",
    "labels_13name = np.delete(labels_allname,remove_pop[0])\n",
    "labels_13 = np.delete(labels_all,remove_pop[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a1b24",
   "metadata": {},
   "source": [
    "### Generating Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b7ff335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haplotype Dataset\n",
    "\n",
    "# X -> complete dataset (haplotypes x SNPs)\n",
    "# Y -> corresponding class(population) labels\n",
    "\n",
    "X = H_13.T.copy()\n",
    "Y = labels_13.copy()\n",
    "\n",
    "# Patterson Scaled\n",
    "X_s = allel.PattersonScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "44a0a702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2234, 117461)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6ff7629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Genotype data  (1117, 117461)\n",
      "The shape of Genotype data labels  (1117,)\n"
     ]
    }
   ],
   "source": [
    "# Generating Genotype (Unphased) Dataset by combining 2 rows of Filtered Haplotype data(Phased)\n",
    "'''\n",
    "The genotype data has 3 values : 0,1,2\n",
    "0 -> 00\n",
    "1 -> 01 or 10\n",
    "2 -> 11\n",
    "\n",
    "1117 Samples across 13 populations.\n",
    "'''\n",
    "\n",
    "X_g = []\n",
    "for i in range(0,len(X),2):\n",
    "    X_g.append(X[i]+X[i+1])\n",
    "X_g = np.array(X_g)\n",
    "\n",
    "print(\"The shape of Genotype data \",X_g.shape)\n",
    "\n",
    "# Generating Genotype labels\n",
    "\n",
    "removeL = range(0,len(Y),2)\n",
    "Y_g = np.delete(Y,removeL)\n",
    "print(\"The shape of Genotype data labels \",Y_g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3aa28247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haplotype Train set: \t (1787, 117461)\n",
      "Haplotype Test set: \t (447, 117461)\n",
      "Genotype Train set: \t (893, 117461)\n",
      "Genotype Test set: \t (224, 117461)\n"
     ]
    }
   ],
   "source": [
    "# Stratified Train-Test Split\n",
    "\n",
    "'''\n",
    "Stratified Splitting : Representation of all the populations is there in test set.\n",
    "\n",
    "Data split : Train = 80%, Test = 20%\n",
    "x_train and y_train -> training data and training labels\n",
    "x_test and y_test -> testing data and test labels\n",
    "labels : Array of number coded labels\n",
    "'''\n",
    "\n",
    "# For Haplotye dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2,stratify = Y, random_state = R_SEED)\n",
    "\n",
    "# For Genotype dataset\n",
    "x_gtrain, x_gtest, y_gtrain, y_gtest = train_test_split(X_g,Y_g,test_size = 0.2,stratify = Y_g, random_state = R_SEED)\n",
    "\n",
    "\n",
    "print(\"Haplotype Train set: \\t\",x_train.shape)\n",
    "print(\"Haplotype Test set: \\t\",x_test.shape)\n",
    "print(\"Genotype Train set: \\t\",x_gtrain.shape)\n",
    "print(\"Genotype Test set: \\t\",x_gtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad4978",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3bf060cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA model on Haplo data\n",
    "lda = LinearDiscriminantAnalysis(solver='svd',n_components = 10)\n",
    "lda.fit(x_train,y_train)\n",
    "\n",
    "# lda_s = LinearDiscriminantAnalysis(solver='svd',n_components = 10)\n",
    "# lda_s.fit(x_train,y_train)\n",
    "\n",
    "# LDA model on Geno data\n",
    "lda_g = LinearDiscriminantAnalysis()\n",
    "lda_g.fit(x_gtrain,y_gtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0b6a945c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Classification Train Accuracy :  0.6536094012311136\n",
      "LDA Classification Test Accuracy :  0.7740492170022372\n",
      "LDA Classification Test ROC-AUC :  0.9707653642850848\n"
     ]
    }
   ],
   "source": [
    "# we will calculate weighted ROC-AUC in One vs Rest setting\n",
    "y_pred = lda.predict(x_test)\n",
    "print(\"LDA Classification Train Accuracy : \",lda.score(x_train,y_train))\n",
    "print(\"LDA Classification Test Accuracy : \",lda.score(x_test,y_test))\n",
    "print(\"LDA Classification Test ROC-AUC : \",roc_auc_score(y_test,lda.predict_proba(x_test),multi_class=\"ovr\", average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "286622fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Genotype Classification Train Accuracy :  0.620380739081747\n",
      "LDA Genotype Classification Test Accuracy :  0.7008928571428571\n",
      "LDA Genotype Classification Test ROC-AUC :  0.9570049518384719\n"
     ]
    }
   ],
   "source": [
    "# we will calculate weighted ROC-AUC in One vs Rest setting\n",
    "y_pred = lda_g.predict(x_gtest)\n",
    "print(\"LDA Genotype Classification Train Accuracy : \",lda_g.score(x_gtrain,y_gtrain))\n",
    "print(\"LDA Genotype Classification Test Accuracy : \",lda_g.score(x_gtest,y_gtest))\n",
    "print(\"LDA Genotype Classification Test ROC-AUC : \",roc_auc_score(y_gtest,lda_g.predict_proba(x_gtest),multi_class=\"ovr\", average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4491bd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Classification Train Accuracy :  0.640695067264574\n",
      "LDA Classification Test Accuracy :  0.7718120805369127\n",
      "LDA Classification Test ROC-AUC :  0.9725032493323238\n"
     ]
    }
   ],
   "source": [
    "# Patterson Scaled Data\n",
    "y_pred = lda_s.predict(x_test)\n",
    "print(\"LDA Classification Train Accuracy : \",lda_s.score(x_train,y_train))\n",
    "print(\"LDA Classification Test Accuracy : \",lda_s.score(x_test,y_test))\n",
    "print(\"LDA Classification Test ROC-AUC : \",roc_auc_score(y_test,lda_s.predict_proba(x_test),multi_class=\"ovr\", average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf034085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(clf,X,Y,folds = 5):\n",
    "    cv_rf = cross_validate(clf,X,Y,scoring=('accuracy','recall','precision'),cv = folds,n_jobs=-1)\n",
    "    print(\"Average CV Accuracy Test \\t%0.2f\"%(cv_rf['test_accuracy'].mean()*100))\n",
    "#     print(\"Average CV ROC-AUC Score \\t%0.2f\"%(cv_rf['test_roc_auc'].mean()*100))\n",
    "    print(\"Average CV Recall Score \\t%0.2f\"%(cv_rf['test_recall'].mean()*100))\n",
    "    print(\"Average CV Precision Score \\t%0.2f\"%(cv_rf['test_precision'].mean()*100))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b29aa2d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Accuracy Test \tnan\n",
      "Average CV Recall Score \tnan\n",
      "Average CV Precision Score \tnan\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# LDA cross validation 10 Fold\n",
    "cross_validation(lda,X,Y,folds = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b84d7",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3dce41ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(n_jobs=-1, verbose=5)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter = 100,n_jobs = -1,verbose=5)\n",
    "lr.fit(x_gtrain,y_gtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "439b25fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Genotype Classification Train Accuracy :  1.0\n",
      "LR Genotype Classification Test Accuracy :  0.9017857142857143\n",
      "LR Genotype Classification Test ROC-AUC :  0.9972696279241168\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(x_gtest)\n",
    "print(\"LR Genotype Classification Train Accuracy : \",lr.score(x_gtrain,y_gtrain))\n",
    "print(\"LR Genotype Classification Test Accuracy : \",lr.score(x_gtest,y_gtest))\n",
    "print(\"LR Genotype Classification Test ROC-AUC : \",roc_auc_score(y_gtest,lr.predict_proba(x_gtest),multi_class=\"ovr\", average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c78d234",
   "metadata": {},
   "source": [
    "#### Exploring a prediction done by LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "974bded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: Class 11 GNgam\n",
      "Predicted as: Class 4 CMgam\n",
      "Probability of Class 0 BFcol: \t0.05442619734035559\n",
      "Probability of Class 1 BFgam: \t13.823098992772367\n",
      "Probability of Class 2 AOcol: \t0.01991886115212178\n",
      "Probability of Class 3 CIcol: \t0.03543322216116989\n",
      "Probability of Class 4 CMgam: \t81.75306593338986\n",
      "Probability of Class 5 FRgam: \t0.015027537943098138\n",
      "Probability of Class 6 GAgam: \t0.07140868451504785\n",
      "Probability of Class 7 GHcol: \t0.025140608742485124\n",
      "Probability of Class 8 GHgam: \t0.07256340081197056\n",
      "Probability of Class 9 GM: \t2.6129413354481104\n",
      "Probability of Class 10 GNcol: \t0.0868522256346968\n",
      "Probability of Class 11 GNgam: \t0.015453965764154287\n",
      "Probability of Class 12 GQgam: \t1.414669034324549\n",
      "--------------------------------------------------\n",
      "Ground Truth: Class 15 UGgam\n",
      "Predicted as: Class 15 UGgam\n",
      "Probability of Class 0 BFcol: \t0.020090664839655895\n",
      "Probability of Class 1 BFgam: \t0.19063705836126446\n",
      "Probability of Class 2 AOcol: \t0.005720600130391184\n",
      "Probability of Class 3 CIcol: \t0.009164317087593871\n",
      "Probability of Class 4 CMgam: \t1.916006498709752\n",
      "Probability of Class 5 FRgam: \t0.0061707427446241365\n",
      "Probability of Class 6 GAgam: \t0.030839313672302957\n",
      "Probability of Class 7 GHcol: \t0.005760611476805049\n",
      "Probability of Class 8 GHgam: \t0.015748532358947437\n",
      "Probability of Class 9 GM: \t0.09003014525153917\n",
      "Probability of Class 10 GNcol: \t0.024963194930903588\n",
      "Probability of Class 11 GNgam: \t0.005277212431902647\n",
      "Probability of Class 12 GQgam: \t97.67959110800432\n",
      "--------------------------------------------------\n",
      "Ground Truth: Class 0 BFcol\n",
      "Predicted as: Class 0 BFcol\n",
      "Probability of Class 0 BFcol: \t92.70625030498735\n",
      "Probability of Class 1 BFgam: \t0.0837992183222891\n",
      "Probability of Class 2 AOcol: \t0.04094274876325859\n",
      "Probability of Class 3 CIcol: \t5.742959201983569\n",
      "Probability of Class 4 CMgam: \t0.10792589334036827\n",
      "Probability of Class 5 FRgam: \t0.007068358966768336\n",
      "Probability of Class 6 GAgam: \t0.02597244385377606\n",
      "Probability of Class 7 GHcol: \t0.6393559307691397\n",
      "Probability of Class 8 GHgam: \t0.07571356460018654\n",
      "Probability of Class 9 GM: \t0.06853163118706536\n",
      "Probability of Class 10 GNcol: \t0.44679865046175016\n",
      "Probability of Class 11 GNgam: \t0.010707657388281328\n",
      "Probability of Class 12 GQgam: \t0.043974395376200445\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k in range(3):\n",
    "    probabs = lr.predict_proba(x_gtest)[k]*100\n",
    "    print(f'Ground Truth: Class {y_gtest[k]} {populations_labels[y_gtest[k]]}')\n",
    "    print(f'Predicted as: Class {y_pred[k]} {populations_labels[y_pred[k]]}')\n",
    "    for i in range(len(probabs)):\n",
    "        print(f'Probability of Class {i} {populations_labels[i]}: \\t{probabs[i]}')\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To Try : \n",
    "\n",
    "Data -> UMAP components (3-10) -> Logistic Regression(Any Model)\n",
    "\n",
    "For inference/predictions the relative components for predictions can be obtained \n",
    "from the frozen UMAP model which in turn can be used on LR model.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
