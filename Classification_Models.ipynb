{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb391712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import allel\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08749988",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~')\n",
    "directory = os.path.join('Imp_Research','Dataset')\n",
    "\n",
    "populations = ['BFcol','BFgam','AOcol','CIcol','CMgam','FRgam',\n",
    "              'GAgam','GHcol','GHgam','GM','GNcol','GNgam','GQgam',\n",
    "              'GW','KE','UGgam']\n",
    "R_SEED = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51cebaf",
   "metadata": {},
   "source": [
    "### Functions to Load and Pre-Process the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1779a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Requirements : NumPy and Scikit-Allel\n",
    "'''\n",
    "\n",
    "class FilterSNP():\n",
    "    def __init__(self,haplotype, POS):\n",
    "        self.haplotype = haplotype\n",
    "        self.POS = POS\n",
    "        self.H = haplotype\n",
    "        self.P = POS\n",
    "        self._removed_maf = None\n",
    "        self._retained_maf = None\n",
    "        self._retained_ld = None\n",
    "        self._removed_ld = None\n",
    "        \n",
    "    def all_filters(self, LD_window_size, LD_overlap_step, MBP_start = 1, MBP_end = 37, MAF_threshold = 5, LD_threshold=.1,LD_iter=1):\n",
    "        print(\"1.) Selecting Mega Base Pairs\")\n",
    "        self.H,self.P = self.get_haplo_MBP(self.H,self.P,start = MBP_start, end = MBP_end)\n",
    "        print(\"MBP selected. Retained Matrix = \", self.H.shape)\n",
    "        print(\"2.) Filtering Rare Allels\")\n",
    "        self.H,self.P = self.filter_MAF(self.H,self.P,threshold = MAF_threshold)\n",
    "        print(\"3.) Performing LD Pruning\")\n",
    "        self.H = self.LD_pruning(self.H, LD_window_size, LD_overlap_step, threshold = LD_threshold, n_iter = LD_iter)\n",
    "        print(\"Retained Matrix = \", self.H.shape)\n",
    "        \n",
    "        return self.H, self.P\n",
    "\n",
    "    def filter_MAF(self,haplo,POS,threshold = 5):\n",
    "        if threshold >= 50 : \n",
    "            print(\"MAF threshold cannot be more than 49%\")\n",
    "            return\n",
    "        samples = haplo.shape[1]\n",
    "        sums = haplo.sum(axis=1)\n",
    "        maf = self.get_MAF(haplo)\n",
    "        #indexes = np.where(maf >= threshold*0.01)[0]\n",
    "        minor = samples*threshold/100\n",
    "        major = samples*(100-threshold)/100\n",
    "        # Selects indexes where allels are >threshold or all 0 and all 1.\n",
    "        indexes = np.where((sums>=minor)& (sums<=major))[0]\n",
    "        print(\"Number of SNPs removed = \",len(haplo)-len(indexes))\n",
    "        print(\"Retaining = \",len(indexes))\n",
    "        self._removed_maf = len(haplo)-len(indexes)\n",
    "        self._retained_maf = len(indexes)\n",
    "        return np.take(haplo,indexes,0), np.take(POS,indexes,0)\n",
    "\n",
    "    # Returns : Array of MAF\n",
    "    def get_MAF(self,haplo):\n",
    "        samples = haplo.shape[1]\n",
    "        sums = haplo.sum(axis=1)\n",
    "        maf = []\n",
    "        for s in sums:\n",
    "            if s != samples or s != 0:\n",
    "                frequency = s/samples\n",
    "                if frequency > 0.5:\n",
    "                    maf.append(1-frequency)\n",
    "                else: \n",
    "                    maf.append(frequency)\n",
    "        return np.array(maf)\n",
    "\n",
    "    def get_MBP(self,POS,start = 1,end = 37):\n",
    "        return np.where(POS[np.where(POS>=1e6)]<=37e6)[0]\n",
    "\n",
    "    def get_haplo_MBP(self,haplotype,POS,start = 1,end = 37):\n",
    "        index = self.get_MBP(POS,start,end)\n",
    "        return np.take(haplotype,index,0),np.take(POS,index,0)\n",
    "    \n",
    "    def LD_pruning(self,gn, size, step, threshold = .1, n_iter=1):\n",
    "        removed = 0\n",
    "        for i in range(n_iter):\n",
    "            loc_unlinked = allel.locate_unlinked(gn, size=size, step=step, threshold=threshold)\n",
    "            n = np.count_nonzero(loc_unlinked)\n",
    "            n_remove = gn.shape[0] - n\n",
    "            removed += n_remove\n",
    "            print('iteration', i+1, 'retaining', n, 'removing', n_remove, 'variants')\n",
    "            gn = gn.compress(loc_unlinked, axis=0)\n",
    "        self._retained_ld = gn.shape[0]\n",
    "        self._removed_ld = removed\n",
    "        return gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e4f0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Disclaimer : The class is for personal use. It is not aimed for portability or reusability.\n",
    "\n",
    "Class to load and filter the data from disk.\n",
    "Object Parameters : \n",
    "data_path  -> Path to the data directory\n",
    "\n",
    "load_pop() : Function to load the data\n",
    "params : \n",
    "Populations -> list or array of population names/filenames\n",
    "filtered -> boolean, whethere to filter the data or not\n",
    "combine -> Boolean, to combine the populations (Implement it !!!)\n",
    "\n",
    "returns : A dictionary of haplotype matrix and Position array\n",
    "'''\n",
    "\n",
    "class LoadFilteredPops():\n",
    "    def __init__(self,data_path = None):\n",
    "        self.data_path = data_path\n",
    "        \n",
    "    def load_pop(self,populations,naming='custom',chromo_arms = ['3R'],filtered = True):\n",
    "        if self.data_path is None:\n",
    "            home = os.path.expanduser('~')\n",
    "            directory = os.path.join('Imp_Research','Dataset')\n",
    "        Haplo_pop = {}\n",
    "        POS_pop = {}\n",
    "        for population in populations:\n",
    "            for arm in chromo_arms:\n",
    "                pop_name = population+'.'+arm\n",
    "                if naming == 'custom':\n",
    "                    filename = f'Haplotype.POS.{pop_name}.hd5'\n",
    "                else:\n",
    "                    filename = population\n",
    "                if self.data_path is None:\n",
    "                    data_path = os.path.join(home, directory,\"HDF_Dataset\", filename)\n",
    "                else:\n",
    "                    try:\n",
    "                        data_path = os.path.join(self.data_path,filename)\n",
    "                    except:\n",
    "                        print(\"Cannot resolve Directory path\")\n",
    "                        exit()\n",
    "                print(f'------{pop_name}------\\n')\n",
    "                H = pd.read_hdf(data_path,key='Haplotype').astype('int8').to_numpy().astype('int8')\n",
    "                P = pd.read_hdf(data_path,key='POS').to_numpy()\n",
    "                \n",
    "                if filtered:\n",
    "                    datafilter = FilterSNP(H,P)\n",
    "                    Haplo_pop[pop_name],POS_pop[pop_name] = datafilter.all_filters(LD_window_size = 500,LD_overlap_step = 100,LD_iter = 3)\n",
    "                else: \n",
    "                    Haplo_pop[pop_name],POS_pop[pop_name] = H,P\n",
    "                del H,P     \n",
    "        print(\"Populations loaded !!!\")\n",
    "        return Haplo_pop,POS_pop\n",
    "\n",
    "    \n",
    "'''\n",
    "Function to combine the populations\n",
    "Params\n",
    "Haplo_all : Dictionary containing Haplotype matrix for populations. Key-> pop name; Value -> matrix.\n",
    "            Matrix dimensions must be POS x haplotypes.\n",
    "POS_all : Position array of SNPs\n",
    "filtered : Boolean, to filter after combining or not.\n",
    "get_labels : Boolean, to generate labels\n",
    "\n",
    "returns\n",
    "Haplo_all : ndarray of n x m dimensions. n = length of POS array; m = sum of haplotypes from all populations.\n",
    "POS : SNP Position array\n",
    "labels : list of labels if Labels = []\n",
    "'''\n",
    "def combine_pops(H_all,P_all,filtered = True,get_labels = True):\n",
    "    keys = list(H_all.keys())\n",
    "    H = np.array(H_all[keys[0]])\n",
    "    if len(H_all) > 1:\n",
    "        for i in range(1,len(H_all)):\n",
    "            H = np.append(H,H_all[keys[i]],axis=1)\n",
    "    print('Combined Shape => ',H.shape)\n",
    "    if get_labels:\n",
    "        label = []\n",
    "        for each in keys:\n",
    "            label.extend([each*len(H_all[each][0]))\n",
    "    if filtered == False:\n",
    "        if get_labels:\n",
    "            return H,P_all,label\n",
    "        else:\n",
    "            return H,p_all\n",
    "    else:\n",
    "        datafilter = FilterSNP(H,P_all)\n",
    "        H_filtered,POS_filtered = datafilter.all_filters(LD_window_size = 500,LD_overlap_step = 100,LD_iter = 3)\n",
    "        print('Filtered Shape => ',H_filtered.shape)\n",
    "        if get_labels:\n",
    "            return H_filtered,POS_filtered,label\n",
    "        else:\n",
    "            return H_filtered,POS_filtered\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e5aaa",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc53edc5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------BFcol.3R------\n",
      "\n",
      "------BFgam.3R------\n",
      "\n",
      "------AOcol.3R------\n",
      "\n",
      "------CIcol.3R------\n",
      "\n",
      "------CMgam.3R------\n",
      "\n",
      "------FRgam.3R------\n",
      "\n",
      "------GAgam.3R------\n",
      "\n",
      "------GHcol.3R------\n",
      "\n",
      "------GHgam.3R------\n",
      "\n",
      "------GM.3R------\n",
      "\n",
      "------GNcol.3R------\n",
      "\n",
      "------GNgam.3R------\n",
      "\n",
      "------GQgam.3R------\n",
      "\n",
      "------GW.3R------\n",
      "\n",
      "------KE.3R------\n",
      "\n",
      "------UGgam.3R------\n",
      "\n",
      "Populations loaded !!!\n"
     ]
    }
   ],
   "source": [
    "loader = LoadFilteredPops()\n",
    "Haplo_all, POS_all = loader.load_pop(populations,filtered = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5b5325b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Shape =>  (4836295, 2284)\n",
      "1.) Selecting Mega Base Pairs\n",
      "MBP selected. Retained Matrix =  (3651720, 2284)\n",
      "2.) Filtering Rare Allels\n",
      "Number of SNPs removed =  3451543\n",
      "Retaining =  200177\n",
      "3.) Performing LD Pruning\n",
      "iteration 1 retaining 117993 removing 82184 variants\n",
      "iteration 2 retaining 117494 removing 499 variants\n",
      "iteration 3 retaining 117461 removing 33 variants\n",
      "Retained Matrix =  (117461, 2284)\n",
      "Filtered Shape =>  (117461, 2284)\n"
     ]
    }
   ],
   "source": [
    "# Combining and Filtering the SNPs\n",
    "H_allF, POS_allF, pop_labels = combine_pops(Haplo_all,POS_all['BFcol.3R'],filtered = True)\n",
    "\n",
    "# To retain the positions simply pass along a list of index values.\n",
    "# Remove the same indexes as POS hence we will know which indexes to use while infering from models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23d9ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Labels\n",
    "\n",
    "labels_all = pd.DataFrame(pop_labels)\n",
    "for i in range(len(populations)):\n",
    "    labels_all[0].replace(populations[i],i,inplace=True)\n",
    "\n",
    "\n",
    "# Name coded labels\n",
    "labels_allname = pd.DataFrame(pop_labels)\n",
    "labels_allname = labels_allname.to_numpy()\n",
    "# Number coded class labels\n",
    "labels_all = labels_all[0].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728c4b4",
   "metadata": {},
   "source": [
    "#### Removing GQgam, GHgam and GNcol due to small sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "602d863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaples to be removed (50,)\n",
      "Samples retained  (117461, 2231)\n"
     ]
    }
   ],
   "source": [
    "H_13 = H_allF.copy()\n",
    "\n",
    "remove_pop = np.where((labels_allname == ['GQgam','GHgam','GNcol']))\n",
    "print(\"Smaples to be removed\",remove_pop[0].shape)\n",
    "\n",
    "H_13 = np.delete(H_13,remove_pop,1)\n",
    "print(\"Samples retained \",H_13.shape)\n",
    "\n",
    "# Generating new labels\n",
    "\n",
    "labels_13name = np.delete(labels_allname,remove_pop)\n",
    "labels_13 = np.delete(labels_all,remove_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ae5bb",
   "metadata": {},
   "source": [
    "### Generating Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b524a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = H_13.T.copy()\n",
    "Y = labels_13.copy()\n",
    "\n",
    "# Patterson Scaled\n",
    "X_s = allel.PattersonScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "31dd22c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (1784, 117461)\n",
      "Test set:  (447, 117461)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train-Test Split\n",
    "'''\n",
    "\n",
    "# Stratified Splitting : Representation of all the populations is there in test set.\n",
    "# Data split : Train = 80%, Test = 20%\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2,stratify = Y, random_state = R_SEED)\n",
    "\n",
    "# x_train and y_train training matrix and training labels\n",
    "# x_test and y_test testing matrix and test labels\n",
    "# labels : Array of number coded labels\n",
    "print(\"Train set: \",x_train.shape)\n",
    "print(\"Test set: \",x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e111a",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fff8366f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=10)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver='svd',n_components = 10)\n",
    "lda.fit(x_train,y_train)\n",
    "\n",
    "# lda_s = LinearDiscriminantAnalysis(solver='svd',n_components = 10)\n",
    "# lda_s.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ebb51ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Classification Train Accuracy :  0.6507847533632287\n",
      "LDA Classification Test Accuracy :  0.7718120805369127\n",
      "LDA Classification Test ROC-AUC :  0.9705182962323551\n"
     ]
    }
   ],
   "source": [
    "# we will calculate weighted ROC-AUC in One vs Rest setting\n",
    "y_pred = lda.predict(x_test)\n",
    "print(\"LDA Classification Train Accuracy : \",lda.score(x_train,y_train))\n",
    "print(\"LDA Classification Test Accuracy : \",lda.score(x_test,y_test))\n",
    "print(\"LDA Classification Test ROC-AUC : \",roc_auc_score(y_test,lda.predict_proba(x_test),multi_class=\"ovr\", average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf5598ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Classification Train Accuracy :  0.640695067264574\n",
      "LDA Classification Test Accuracy :  0.7718120805369127\n",
      "LDA Classification Test ROC-AUC :  0.9725032493323238\n"
     ]
    }
   ],
   "source": [
    "# Patterson Scaled Data\n",
    "y_pred = lda_s.predict(x_test)\n",
    "print(\"LDA Classification Train Accuracy : \",lda_s.score(x_train,y_train))\n",
    "print(\"LDA Classification Test Accuracy : \",lda_s.score(x_test,y_test))\n",
    "print(\"LDA Classification Test ROC-AUC : \",roc_auc_score(y_test,lda_s.predict_proba(x_test),multi_class=\"ovr\", average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7dd2fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(clf,X,Y,folds = 5):\n",
    "    cv_rf = cross_validate(clf,X,Y,scoring=('accuracy','recall','precision'),cv = folds,n_jobs=-1)\n",
    "    print(\"Average CV Accuracy Test \\t%0.2f\"%(cv_rf['test_accuracy'].mean()*100))\n",
    "#     print(\"Average CV ROC-AUC Score \\t%0.2f\"%(cv_rf['test_roc_auc'].mean()*100))\n",
    "    print(\"Average CV Recall Score \\t%0.2f\"%(cv_rf['test_recall'].mean()*100))\n",
    "    print(\"Average CV Precision Score \\t%0.2f\"%(cv_rf['test_precision'].mean()*100))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9f4d7f85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Accuracy Test \tnan\n",
      "Average CV Recall Score \tnan\n",
      "Average CV Precision Score \tnan\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# LDA cross validation 10 Fold\n",
    "cross_validation(lda,X,Y,folds = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda6ada",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f624bc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achal/miniconda3/envs/Imp-Res/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1357: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 56.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, n_jobs=-1, solver='liblinear', verbose=5)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver = 'liblinear',max_iter = 1000,n_jobs = -1,verbose=5)\n",
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lda.predict(x_test)\n",
    "print(\"LR Classification Train Accuracy : \",lr.score(x_train,y_train))\n",
    "print(\"LR Classification Test Accuracy : \",lr.score(x_test,y_test))\n",
    "print(\"LR Classification Test ROC-AUC : \",roc_auc_score(y_test,lr.predict_proba(x_test),multi_class=\"ovr\", average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To Try : \n",
    "\n",
    "Data -> UMAP components (3-10) -> Logistic Regression(Any Model)\n",
    "\n",
    "For inference/predictions the relative components for predictions can be obtained \n",
    "from the frozen UMAP model which in turn can be used on LR model.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
