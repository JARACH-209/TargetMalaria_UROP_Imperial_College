{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d858fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import allel\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_score, accuracy_score, cohen_kappa_score, f1_score\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8209350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_SEED = 22\n",
    "home = os.path.expanduser('~')\n",
    "directory = os.path.join('Imp_Research','Dataset')\n",
    "\n",
    "# populations = ['BFcol','BFgam','AOcol','CIcol','CMgam','FRgam',\n",
    "#               'GAgam','GHcol','GHgam','GM','GNcol','GNgam','GQgam',\n",
    "#               'GW','KE','UGgam']\n",
    "\n",
    "# Using only 12 populations out of 16\n",
    "populations = ['BFcol','BFgam','AOcol','CIcol','CMgam',\n",
    "              'GAgam','GHcol','GM','GNgam','GW','KE','UGgam']\n",
    "\n",
    "# Creating a dictionary of Population names with labels\n",
    "populations_encoding = {}\n",
    "for i in range(len(populations)):\n",
    "    populations_encoding[i] = populations[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befeffb4",
   "metadata": {},
   "source": [
    "### Functions to Load and Pre-Process the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc12336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Requirements : NumPy and Scikit-Allel\n",
    "'''\n",
    "\n",
    "class FilterSNP():\n",
    "    def __init__(self,haplotype, POS):\n",
    "        self.haplotype = haplotype\n",
    "        self.POS = POS\n",
    "        self.H = haplotype\n",
    "        self.P = POS\n",
    "        self._removed_maf = None\n",
    "        self._retained_maf = None\n",
    "        self._retained_ld = None\n",
    "        self._removed_ld = None\n",
    "        self._unlinked_POS = None\n",
    "        \n",
    "    def all_filters(self, LD_window_size, LD_overlap_step, MBP_start = 1, MBP_end = 37, MAF_threshold = 5, LD_threshold=.1,LD_iter=1):\n",
    "        print(\"1.) Selecting Mega Base Pairs\")\n",
    "        self.H,self.P = self.get_haplo_MBP(self.H,self.P,start = MBP_start, end = MBP_end)\n",
    "        print(\"MBP selected. Retained Matrix = \", self.H.shape)\n",
    "        print(\"2.) Filtering Rare Allels\")\n",
    "        self.H,self.P = self.filter_MAF(self.H,self.P,threshold = MAF_threshold)\n",
    "        print(\"3.) Performing LD Pruning\")\n",
    "        self.H,self.P = self.LD_pruning(self.H, self.P, LD_window_size, LD_overlap_step, threshold = LD_threshold, n_iter = LD_iter)\n",
    "        print(\"Retained Haplotype Matrix = \", self.H.shape)\n",
    "        print(\"Retained Positions Matrix = \",self.P.shape)\n",
    "        \n",
    "        return self.H, self.P\n",
    "        \n",
    "    # input dimensions for Haplotype matrix (gn) = SNP X n\n",
    "    def filter_MAF(self,haplo,POS,threshold = 5):\n",
    "        if threshold >= 50 : \n",
    "            print(\"MAF threshold cannot be more than 49%\")\n",
    "            return\n",
    "        samples = haplo.shape[1]\n",
    "        sums = haplo.sum(axis=1)\n",
    "        maf = self.get_MAF(haplo)\n",
    "        #indexes = np.where(maf >= threshold*0.01)[0]\n",
    "        minor = samples*threshold/100\n",
    "        major = samples*(100-threshold)/100\n",
    "        # Selects indexes where allels are >threshold or all 0 and all 1.\n",
    "        indexes = np.where((sums>=minor)& (sums<=major))[0]\n",
    "        print(\"Number of SNPs removed = \",len(haplo)-len(indexes))\n",
    "        print(\"Retaining = \",len(indexes))\n",
    "        self._removed_maf = len(haplo)-len(indexes)\n",
    "        self._retained_maf = len(indexes)\n",
    "        return np.take(haplo,indexes,0), np.take(POS,indexes,0)\n",
    "\n",
    "    # Returns : Array of MAF\n",
    "    # input dimensions for Haplotype matrix (gn) = SNP X n\n",
    "    def get_MAF(self,haplo):\n",
    "        samples = haplo.shape[1]\n",
    "        sums = haplo.sum(axis=1)\n",
    "        maf = []\n",
    "        for s in sums:\n",
    "            if s != samples or s != 0:\n",
    "                frequency = s/samples\n",
    "                if frequency > 0.5:\n",
    "                    maf.append(1-frequency)\n",
    "                else: \n",
    "                    maf.append(frequency)\n",
    "        return np.array(maf)\n",
    "\n",
    "    # input dimensions for Haplotype matrix (gn) = SNP X n\n",
    "    def get_MBP(self,POS,start = 1,end = 37):\n",
    "        return np.where(POS[np.where(POS>=1e6)]<=37e6)[0]\n",
    "\n",
    "    # input dimensions for Haplotype matrix (gn) = SNP X n\n",
    "    def get_haplo_MBP(self,haplotype,POS,start = 1,end = 37):\n",
    "        index = self.get_MBP(POS,start,end)\n",
    "        return np.take(haplotype,index,0),np.take(POS,index,0)\n",
    "    \n",
    "    # input dimensions for Haplotype matrix (gn) = SNP X n\n",
    "    def LD_pruning(self, gn, pos, size, step, threshold = .1, n_iter=1):\n",
    "        removed = 0\n",
    "        for i in range(n_iter):\n",
    "            \n",
    "            # Returns a boolean array. True(1) - SNPs are within threshold (Unliked), False(0) = Linked\n",
    "            loc_unlinked = allel.locate_unlinked(gn, size=size, step=step, threshold=threshold)\n",
    "            \n",
    "            # Counts non-zero elements = No. of unlinked loci or SNPs to retain\n",
    "            n = np.count_nonzero(loc_unlinked)\n",
    "            \n",
    "            # Calculate the number of SNPs to be removed\n",
    "            n_remove = gn.shape[0] - n\n",
    "            removed += n_remove\n",
    "            print('iteration', i+1, 'retaining', n, 'removing', n_remove, 'variants')\n",
    "            \n",
    "            # Select only the unlinked SNPs, i.e., indexes where value is True(1).\n",
    "            gn = gn.compress(loc_unlinked, axis=0)\n",
    "            \n",
    "            # retaining the indexes preserved\n",
    "            pos = pos.compress(loc_unlinked)\n",
    "        self._retained_ld = gn.shape[0]\n",
    "        self._removed_ld = removed\n",
    "        self._unlinked_POS = pos\n",
    "        return gn,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6856ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Disclaimer : The class is for personal use. It is not aimed for portability or reusability.\n",
    "\n",
    "Class to load and filter the data from disk.\n",
    "Object Parameters : \n",
    "data_path  -> Path to the data directory\n",
    "\n",
    "load_pop() : Function to load the data\n",
    "params : \n",
    "Populations -> list or array of population names/filenames\n",
    "filtered -> boolean, whethere to filter the data or not\n",
    "combine -> Boolean, to combine the populations (Implement it !!!)\n",
    "\n",
    "returns : A dictionary of haplotype matrix and Position array\n",
    "'''\n",
    "\n",
    "class LoadFilteredPops():\n",
    "    def __init__(self,data_path = None):\n",
    "        self.data_path = data_path\n",
    "        \n",
    "    def load_pop(self,populations,naming='custom',chromo_arms = ['3R'],filtered = True):\n",
    "        if self.data_path is None:\n",
    "            home = os.path.expanduser('~')\n",
    "            directory = os.path.join('Imp_Research','Dataset')\n",
    "        Haplo_pop = {}\n",
    "        POS_pop = {}\n",
    "        for population in populations:\n",
    "            for arm in chromo_arms:\n",
    "                pop_name = population+'.'+arm\n",
    "                if naming == 'custom':\n",
    "                    filename = f'Haplotype.POS.{pop_name}.hd5'\n",
    "                else:\n",
    "                    filename = population\n",
    "                if self.data_path is None:\n",
    "                    data_path = os.path.join(home, directory,\"HDF_Dataset\", filename)\n",
    "                else:\n",
    "                    try:\n",
    "                        data_path = os.path.join(self.data_path,filename)\n",
    "                    except:\n",
    "                        print(\"Cannot resolve Directory path\")\n",
    "                        exit()\n",
    "                print(f'------{pop_name}------\\n')\n",
    "                H = pd.read_hdf(data_path,key='Haplotype').astype('int8').to_numpy().astype('int8')\n",
    "                P = pd.read_hdf(data_path,key='POS').to_numpy()\n",
    "                \n",
    "                if filtered:\n",
    "                    # Uses FilterSNP class for filtering steps\n",
    "                    datafilter = FilterSNP(H,P)\n",
    "                    Haplo_pop[pop_name],POS_pop[pop_name] = datafilter.all_filters(LD_window_size = 500,LD_overlap_step = 100,LD_iter = 3)\n",
    "                else: \n",
    "                    Haplo_pop[pop_name],POS_pop[pop_name] = H,P\n",
    "                del H,P     \n",
    "        print(\"Populations loaded !!!\")\n",
    "        return Haplo_pop,POS_pop\n",
    "\n",
    "    \n",
    "'''\n",
    "Function to combine the populations\n",
    "Input Params\n",
    "Haplo_all : Dictionary containing Haplotype matrix for populations. Key-> pop name; Value -> matrix.\n",
    "            Matrix dimensions must be POS x haplotypes.\n",
    "POS_all : Position array of SNPs\n",
    "filtered : Boolean, to filter after combining or not.\n",
    "get_labels : Boolean, to generate labels\n",
    "\n",
    "returns\n",
    "Haplo_all : ndarray of n x m dimensions. n = length of POS array; m = sum of haplotypes from all populations.\n",
    "POS : SNP Position array\n",
    "labels : list of labels if Labels = []\n",
    "'''\n",
    "\n",
    "# def all_filters(self, LD_window_size, LD_overlap_step, MBP_start = 1, \n",
    "#                 MBP_end = 37, MAF_threshold = 5, LD_threshold=.1,LD_iter=1):\n",
    "\n",
    "def combine_pops(H_all,P_all,filtered = True,get_labels = True,LD_window_size = 500, LD_overlap_step = 100,\n",
    "                 MBP_start = 1, MBP_end = 37, MAF_threshold = 5, LD_threshold=.1,LD_iter=3):\n",
    "    keys = list(H_all.keys())\n",
    "    \n",
    "    # Unpacking the Haplotype matrices of all pops from dictionary\n",
    "    H = np.array(H_all[keys[0]])\n",
    "    if len(H_all) > 1:\n",
    "        for i in range(1,len(H_all)):\n",
    "            H = np.append(H,H_all[keys[i]],axis=1)\n",
    "    print('Combined Shape => ',H.shape)\n",
    "    # Generating Labels if True\n",
    "    if get_labels:\n",
    "        label = []\n",
    "        for each in keys:\n",
    "            label.extend([each]*len(H_all[each][0]))\n",
    "                          \n",
    "    # Returning the combined Haplotype data matrix if Filtered = False\n",
    "    if filtered == False:\n",
    "        if get_labels:\n",
    "            return H,P_all,label\n",
    "        else:\n",
    "            return H,p_all\n",
    "    else:\n",
    "        # Uses FilterSNP class for filtering steps\n",
    "        datafilter = FilterSNP(H,P_all)\n",
    "        H_filtered,POS_filtered = datafilter.all_filters(LD_window_size,LD_overlap_step,MBP_start,MBP_end,\n",
    "                                                            MAF_threshold,LD_threshold,LD_iter)\n",
    "        print('Final Filtered Shape = ',H_filtered.shape)\n",
    "        if get_labels:\n",
    "            return H_filtered,POS_filtered,label\n",
    "        else:\n",
    "            return H_filtered,POS_filtered\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f077a",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c07c8b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------BFcol.3R------\n",
      "\n",
      "------BFgam.3R------\n",
      "\n",
      "------AOcol.3R------\n",
      "\n",
      "------CIcol.3R------\n",
      "\n",
      "------CMgam.3R------\n",
      "\n",
      "------GAgam.3R------\n",
      "\n",
      "------GHcol.3R------\n",
      "\n",
      "------GM.3R------\n",
      "\n",
      "------GNgam.3R------\n",
      "\n",
      "------GW.3R------\n",
      "\n",
      "------KE.3R------\n",
      "\n",
      "------UGgam.3R------\n",
      "\n",
      "Populations loaded !!!\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset into the memory using the custom loader classes above\n",
    "# Stored into dictionaries\n",
    "\n",
    "loader = LoadFilteredPops()\n",
    "Haplo_all, POS_all = loader.load_pop(populations,filtered = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42304c05",
   "metadata": {},
   "source": [
    "#### Downsampling CMgam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7041cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of CMgam:  (4836295, 200)\n"
     ]
    }
   ],
   "source": [
    "H = Haplo_all.copy()\n",
    "\n",
    "# Find the indexes  of CMgam\n",
    "cmgam = H['CMgam.3R']\n",
    "# Retaining only first 100 samples (200 Haplotypes)\n",
    "H['CMgam.3R'] = cmgam[:,:200]\n",
    "print('Shape of CMgam: ',H['CMgam.3R'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309454c",
   "metadata": {},
   "source": [
    "#### Converting Haplotypes into Genotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65d2c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFcol.3R transformed shape: \t(75, 4836295)\n",
      "BFgam.3R transformed shape: \t(92, 4836295)\n",
      "AOcol.3R transformed shape: \t(78, 4836295)\n",
      "CIcol.3R transformed shape: \t(71, 4836295)\n",
      "CMgam.3R transformed shape: \t(100, 4836295)\n",
      "GAgam.3R transformed shape: \t(69, 4836295)\n",
      "GHcol.3R transformed shape: \t(55, 4836295)\n",
      "GM.3R transformed shape: \t(65, 4836295)\n",
      "GNgam.3R transformed shape: \t(40, 4836295)\n",
      "GW.3R transformed shape: \t(91, 4836295)\n",
      "KE.3R transformed shape: \t(48, 4836295)\n",
      "UGgam.3R transformed shape: \t(112, 4836295)\n"
     ]
    }
   ],
   "source": [
    "for key in H.keys():\n",
    "    X = H[key].T\n",
    "    X_g = []\n",
    "    for i in range(0,len(X),2):\n",
    "        X_g.append(X[i]+X[i+1])\n",
    "    X_g = np.array(X_g)\n",
    "    H[key] = X_g\n",
    "    print(f'{key} transformed shape: \\t{X_g.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50eecba",
   "metadata": {},
   "source": [
    "### Analysis Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85f1d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_genotype(X):\n",
    "    Xg = []\n",
    "    for i in range(0,len(X),2):\n",
    "        Xg.append(X[i]+X[i+1])\n",
    "    return np.array(Xg)\n",
    "\n",
    "def evaluate_classifier(y_true,y_pred):\n",
    "    test_accuracy = accuracy_score(y_true,y_pred)\n",
    "    f1w = f1_score(y_true,y_pred,average= 'weighted')\n",
    "    kappa = cohen_kappa_score(y_true,y_pred)\n",
    "    return test_accuracy, f1w, kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b02c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFcol_BFgam\n",
      "4836295\n",
      "1.) Selecting Mega Base Pairs\n",
      "MBP selected. Retained Matrix =  (3651720, 334)\n",
      "2.) Filtering Rare Allels\n",
      "Number of SNPs removed =  3448792\n",
      "Retaining =  202928\n",
      "3.) Performing LD Pruning\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Pairwise analysis of all the 12 populations.\n",
    "Total 66 combinations/analysis\n",
    "Analysis includes:\n",
    "1. Selecting a pair\n",
    "2. Filtering the SNPs jointly\n",
    "    - Mega Base Pair Selection\n",
    "    - Rare Allele Filtering with MAF >= 5%\n",
    "    - LD Pruning\n",
    "    Store Filtering stats for all the pairs\n",
    "3. Haplo to Geno and Train-Test Split\n",
    "4. Classifying using Logistic Regression and LDA\n",
    "5. Generating UMAP visualization\n",
    "6. Dimensionality Reduction: Obtaining 20 UMAP components \n",
    "7. Training LR classifier on UMAP components\n",
    "8. Dumping all the stats and evaluation scores into a dataframe\n",
    "'''\n",
    "# Storage Path\n",
    "home = home = os.path.expanduser('~')\n",
    "directory = os.path.join(home,'Imp_Research','Repository','Pairwise_Analysis')\n",
    "\n",
    "# List of keys\n",
    "keys = list(H.keys())\n",
    "count = 0\n",
    "pnames = []\n",
    "psamples = []\n",
    "pmaf = []\n",
    "psnps_init = []\n",
    "psnps_filt = []\n",
    "accuracies_lr = []\n",
    "f1scores_lr = []\n",
    "kappas_lr = []\n",
    "accuracies_ulr = []\n",
    "f1scores_ulr = []\n",
    "kappas_ulr = []\n",
    "\n",
    "\n",
    "for i in range(11):\n",
    "    for j in range(i+1,12):\n",
    "        count+=1\n",
    "        pair_pos = POS_all[keys[i]]\n",
    "        # 1. Pair Selection. Combining the populations. \n",
    "        pop1 = H[keys[i]]\n",
    "        pop2 = H[keys[j]]\n",
    "        pair_h = np.append(pop1,pop2,1)\n",
    "        pname = f'{keys[i][:-3]}_{keys[j][:-3]}'\n",
    "        psnp_init = pair_h.shape[0]\n",
    "        psample = pair_h.shape[1]\n",
    "        print(f'{pname}\\n{psnp_init}')\n",
    "        \n",
    "        # 2. Filtering (Using FilterSNP class) Taking window size 250 is quicker.\n",
    "        fsnp = FilterSNP(pair_h,pair_pos)\n",
    "        pair_h, pair_pos = fsnp.all_filters(LD_window_size = 2000,LD_overlap_step = 200,LD_iter = 3)\n",
    "        psnp_filt = pair_h.shape[0]\n",
    "        pmaf.append(fsnp._retained_maf)\n",
    "#         ppos_filt = len(pair_pos)\n",
    "        \n",
    "        # 3. Haplo to Geno and Train-Test Split\n",
    "        X = pair_h.T\n",
    "        Xg = to_genotype(X)       #Genotype data\n",
    "        pshape_g = Xg.shape\n",
    "        Y = [0]*int(len(pop1[0])/2)\n",
    "        Y.extend([1]*int(len(pop2[0])/2))\n",
    "        Y = np.array(Y)\n",
    "        x_train,x_test,y_train,y_test = train_test_split(Xg,Y,test_size = 0.8,stratify = Y,random_state= R_SEED)\n",
    "        \n",
    "        # 4. LR Classifier\n",
    "        lr = LogisticRegression(n_jobs = -1)\n",
    "        lr.fit(x_train,y_train)\n",
    "        acc_lr, f1w_lr, kappa_lr = evaluate_classifier(y_test,lr.predict(x_test))\n",
    "        \n",
    "        # 5. Generate UMAP visualization\n",
    "        UMAP_g = umap.UMAP(n_neighbors=15,metric='euclidean',min_dist=2,spread=5).fit_transform(Xg)\n",
    "        sns.set(style='white')\n",
    "        fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "        plt.scatter(UMAP_g[:, 0],UMAP_g[:, 1],c=Y,cmap='coolwarm')\n",
    "        plt.setp(ax, xticks=[], yticks=[])\n",
    "        cbar = plt.colorbar(boundaries=np.arange(3)-0.5)\n",
    "        cbar.set_ticks(np.arange(2))\n",
    "        cbar.set_ticklabels(populations)\n",
    "        plt.title(f'Genotype populations {pname} UMAP. metric = euclidean,neighbors = 15,min-dist = 2,spread = 5');\n",
    "        plt.show()\n",
    "        figname = f'{count}_{pname}_umap.png'\n",
    "        path = os.path.join(directory,'pairwise_plots',figname)\n",
    "        plt.savefig(path,dpi=300)\n",
    "        \n",
    "        # 6. Dimensionality Reduction: Obtaining 20 umap components\n",
    "        UMAP_g = umap.UMAP(n_components = 20,n_neighbors=15,metric='euclidean',min_dist=2,spread=5).fit(x_train)\n",
    "        x_train = UMAP_g.embedding_\n",
    "        x_test = UMAP_g.transform(x_test)\n",
    "        \n",
    "        # 7. Training LR on UMAP components\n",
    "        ulr = LogisticRegression(n_jobs = -1)\n",
    "        ulr.fit(x_train,y_train)\n",
    "        acc_ulr, f1w_ulr, kappa_ulr = evaluate_classifier(y_test,ulr.predict(x_test))\n",
    "    \n",
    "    pnames.append(pname)\n",
    "    psamples.append(psample)\n",
    "    psnps_init.append(psnp_init)\n",
    "    psnps_filt.append(psnp_filt)\n",
    "    accuracies_lr.append(acc_lr)\n",
    "    f1scores_lr.append(f1w_lr)\n",
    "    kappas_lr.append(kappa_lr)\n",
    "    accuracies_ulr.append(acc_ulr)\n",
    "    f1scores_ulr.append(f1w_ulr)\n",
    "    kappas_ulr.append(kappa_ulr)\n",
    "    \n",
    "pairwise_analysis = pd.DataFrame()\n",
    "pairwise_analysis['Count'] = range(1,len(pnames)+1)\n",
    "pairwise_analysis['Population_pair'] = pnames\n",
    "pairwise_analysis['Samples'] = psamples\n",
    "pairwise_analysis['Initial_SNP'] = psnps_init\n",
    "pairwise_analysis['MAF_filter_SNP'] = pmaf\n",
    "pairwise_analysis['Filtered_SNP'] = psnps_filt\n",
    "pairwise_analysis['LR_accuracy'] = accuracies_lr\n",
    "pairwise_analysis['LR_f1score'] = f1scores_lr\n",
    "pairwise_analysis['LR_kappa'] = kappas_lr\n",
    "pairwise_analysis['UMAP_LR_accuracy'] = accuracies_ulr\n",
    "pairwise_analysis['UMAP_LR_f1score'] = f1scores_ulr\n",
    "pairwise_analysis['UMAP_LR_kappa'] = kappas_ulr\n",
    "\n",
    "print(pairwise_analysis)\n",
    "path = os.path.join(directory,'Pairwise_Analysis_Stats.csv')\n",
    "pairwise_analysis.to_csv(path,index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "57c99b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Population_pair</th>\n",
       "      <th>Samples</th>\n",
       "      <th>Initial_SNP</th>\n",
       "      <th>MAF_filter_SNP</th>\n",
       "      <th>Filtered_SNP</th>\n",
       "      <th>LR_accuracy</th>\n",
       "      <th>LR_f1score</th>\n",
       "      <th>LR_kappa</th>\n",
       "      <th>UMAP_LR_accuracy</th>\n",
       "      <th>UMAP_LR_f1score</th>\n",
       "      <th>UMAP_LR_kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BFcol_BFgam</td>\n",
       "      <td>334</td>\n",
       "      <td>4836295</td>\n",
       "      <td>202928</td>\n",
       "      <td>119841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count Population_pair  Samples  Initial_SNP  MAF_filter_SNP  Filtered_SNP  \\\n",
       "0    NaN     BFcol_BFgam      334      4836295          202928        119841   \n",
       "\n",
       "   LR_accuracy  LR_f1score  LR_kappa  UMAP_LR_accuracy  UMAP_LR_f1score  \\\n",
       "0          1.0         1.0       1.0               1.0              1.0   \n",
       "\n",
       "   UMAP_LR_kappa  \n",
       "0            1.0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_analysis.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
